"""
Model Monitoring and Alerting Service

Monitors ML model performance, drift detection, and system health.
Implements continuous monitoring for production ML systems.

Key responsibilities:
- Prediction distribution monitoring
- Performance metric tracking
- Data drift detection
- Model staleness detection
- Automated alerting
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Optional
import logging
import json

import numpy as np
from scipy import stats

logger = logging.getLogger(__name__)


class AlertSeverity(str, Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class MetricType(str, Enum):
    """Types of monitored metrics"""
    PREDICTION_DISTRIBUTION = "prediction_distribution"
    FEATURE_DRIFT = "feature_drift"
    PERFORMANCE_DRIFT = "performance_drift"
    DATA_QUALITY = "data_quality"
    SYSTEM_HEALTH = "system_health"
    FAIRNESS = "fairness"


@dataclass
class MonitoringAlert:
    """An alert generated by the monitoring system"""
    alert_id: str
    timestamp: datetime
    severity: AlertSeverity
    metric_type: MetricType
    title: str
    description: str
    metric_value: float
    threshold: float
    recommended_action: str
    affected_tenants: list[str] = field(default_factory=list)
    metadata: dict = field(default_factory=dict)


@dataclass
class HealthCheckResult:
    """Result of a health check"""
    check_name: str
    status: str  # "healthy", "degraded", "unhealthy"
    message: str
    details: dict = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.utcnow)


@dataclass
class DriftReport:
    """Report on detected data or model drift"""
    report_id: str
    generated_at: datetime
    period_start: datetime
    period_end: datetime
    
    # Drift scores (0-1, higher = more drift)
    feature_drift_score: float
    prediction_drift_score: float
    
    # Detailed drift by feature
    feature_drift_details: dict[str, float]
    
    # Alerting
    requires_attention: bool
    alerts: list[MonitoringAlert]
    
    # Recommendations
    recommendations: list[str]


class ModelMonitoringService:
    """
    Comprehensive monitoring for ML model health.
    
    Monitors:
    1. Prediction distributions - are outputs reasonable?
    2. Feature drift - have inputs changed significantly?
    3. Performance drift - is accuracy declining?
    4. Data quality - are there data issues?
    5. Fairness - are predictions equitable?
    """
    
    # Thresholds for alerting
    CRITICAL_DRIFT_THRESHOLD = 0.3
    WARNING_DRIFT_THRESHOLD = 0.15
    CRITICAL_DISTRIBUTION_CHANGE = 0.25
    MAX_PREDICTION_LATENCY_MS = 500
    MIN_DAILY_PREDICTIONS = 100
    
    def __init__(self, db_connection, metrics_client=None):
        self.db = db_connection
        self.metrics = metrics_client
        self._baseline_distributions: dict[str, Any] = {}
    
    async def run_health_check(self) -> list[HealthCheckResult]:
        """
        Run comprehensive health check on the ML system.
        
        Returns list of health check results for each component.
        """
        results = []
        
        # Check model availability
        results.append(await self._check_model_availability())
        
        # Check prediction latency
        results.append(await self._check_prediction_latency())
        
        # Check database connectivity
        results.append(await self._check_database_health())
        
        # Check prediction volume
        results.append(self._check_prediction_volume())
        
        # Check feature store
        results.append(await self._check_feature_store())
        
        # Log results
        for result in results:
            if result.status == "unhealthy":
                logger.error(f"Health check failed: {result.check_name} - {result.message}")
            elif result.status == "degraded":
                logger.warning(f"Health check degraded: {result.check_name} - {result.message}")
        
        return results
    
    def detect_drift(
        self,
        tenant_id: Optional[str] = None,
        lookback_days: int = 7,
    ) -> DriftReport:
        """
        Detect drift in features and predictions.
        
        Compares recent data against baseline to detect:
        - Feature distribution changes
        - Prediction distribution shifts
        - Concept drift indicators
        """
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=lookback_days)
        
        # Get recent predictions and features
        recent_data = self._get_recent_prediction_data(
            tenant_id, start_date, end_date
        )
        
        # Get baseline data (from model training or earlier period)
        baseline_data = self._get_baseline_data(tenant_id)
        
        if not recent_data or not baseline_data:
            return DriftReport(
                report_id=f"drift_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}",
                generated_at=datetime.now(timezone.utc),
                period_start=start_date,
                period_end=end_date,
                feature_drift_score=0.0,
                prediction_drift_score=0.0,
                feature_drift_details={},
                requires_attention=False,
                alerts=[],
                recommendations=["Insufficient data for drift detection"],
            )
        
        # Calculate feature drift
        feature_drift_details = self._calculate_feature_drift(
            recent_data["features"],
            baseline_data["features"],
        )
        
        overall_feature_drift = np.mean(list(feature_drift_details.values()))
        
        # Calculate prediction drift
        prediction_drift = self._calculate_prediction_drift(
            recent_data["predictions"],
            baseline_data["predictions"],
        )
        
        # Generate alerts
        alerts = []
        recommendations = []
        
        # Check for critical feature drift
        high_drift_features = [
            f for f, d in feature_drift_details.items()
            if d > self.CRITICAL_DRIFT_THRESHOLD
        ]
        
        if high_drift_features:
            alerts.append(MonitoringAlert(
                alert_id=f"drift_feature_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}",
                timestamp=datetime.now(timezone.utc),
                severity=AlertSeverity.ERROR,
                metric_type=MetricType.FEATURE_DRIFT,
                title="Critical Feature Drift Detected",
                description=(
                    f"Significant drift detected in features: {', '.join(high_drift_features)}. "
                    "Model predictions may be unreliable."
                ),
                metric_value=overall_feature_drift,
                threshold=self.CRITICAL_DRIFT_THRESHOLD,
                recommended_action=(
                    "1. Review data pipeline for issues\n"
                    "2. Consider retraining model with recent data\n"
                    "3. Investigate root cause of distribution change"
                ),
                affected_tenants=[tenant_id] if tenant_id else [],
            ))
            recommendations.append(
                f"Investigate drift in features: {', '.join(high_drift_features)}"
            )
            recommendations.append("Consider retraining model with recent data")
        
        # Check for prediction drift
        if prediction_drift > self.CRITICAL_DISTRIBUTION_CHANGE:
            alerts.append(MonitoringAlert(
                alert_id=f"drift_pred_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}",
                timestamp=datetime.now(timezone.utc),
                severity=AlertSeverity.WARNING,
                metric_type=MetricType.PREDICTION_DISTRIBUTION,
                title="Prediction Distribution Shift",
                description=(
                    f"Prediction distribution has shifted significantly "
                    f"(drift score: {prediction_drift:.2f}). This may indicate "
                    "concept drift or data quality issues."
                ),
                metric_value=prediction_drift,
                threshold=self.CRITICAL_DISTRIBUTION_CHANGE,
                recommended_action=(
                    "1. Compare prediction distributions across time\n"
                    "2. Check for data quality issues\n"
                    "3. Validate model performance on recent labeled data"
                ),
                affected_tenants=[tenant_id] if tenant_id else [],
            ))
            recommendations.append("Validate model performance on recent labeled data")
        
        requires_attention = len(alerts) > 0
        
        if not recommendations:
            recommendations.append("No significant drift detected. Continue routine monitoring.")
        
        report = DriftReport(
            report_id=f"drift_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}",
            generated_at=datetime.now(timezone.utc),
            period_start=start_date,
            period_end=end_date,
            feature_drift_score=overall_feature_drift,
            prediction_drift_score=prediction_drift,
            feature_drift_details=feature_drift_details,
            requires_attention=requires_attention,
            alerts=alerts,
            recommendations=recommendations,
        )
        
        # Store report
        self._store_drift_report(report)
        
        # Send alerts if needed
        for alert in alerts:
            self._send_alert(alert)
        
        return report
    
    def _calculate_feature_drift(
        self,
        recent_features: dict[str, np.ndarray],
        baseline_features: dict[str, np.ndarray],
    ) -> dict[str, float]:
        """
        Calculate drift score for each feature using PSI (Population Stability Index).
        
        PSI interpretation:
        - < 0.1: No significant change
        - 0.1 - 0.2: Slight change, monitor
        - > 0.2: Significant change, investigate
        """
        drift_scores = {}
        
        for feature_name in recent_features:
            if feature_name not in baseline_features:
                continue
            
            recent = recent_features[feature_name]
            baseline = baseline_features[feature_name]
            
            # Calculate PSI
            psi = self._calculate_psi(baseline, recent)
            drift_scores[feature_name] = psi
        
        return drift_scores
    
    def _calculate_psi(
        self,
        expected: np.ndarray,
        actual: np.ndarray,
        n_bins: int = 10,
    ) -> float:
        """
        Calculate Population Stability Index.
        
        PSI = Σ (Actual% - Expected%) × ln(Actual% / Expected%)
        """
        # Create bins from expected distribution
        try:
            _, bin_edges = np.histogram(expected, bins=n_bins)
        except ValueError:
            return 0.0
        
        # Calculate proportions for each bin
        expected_counts, _ = np.histogram(expected, bins=bin_edges)
        actual_counts, _ = np.histogram(actual, bins=bin_edges)
        
        expected_pct = expected_counts / len(expected)
        actual_pct = actual_counts / len(actual)
        
        # Avoid division by zero
        expected_pct = np.clip(expected_pct, 0.0001, None)
        actual_pct = np.clip(actual_pct, 0.0001, None)
        
        # Calculate PSI
        psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))
        
        return float(psi)
    
    def _calculate_prediction_drift(
        self,
        recent_predictions: np.ndarray,
        baseline_predictions: np.ndarray,
    ) -> float:
        """Calculate drift in prediction distribution using KS test"""
        try:
            ks_stat, _ = stats.ks_2samp(baseline_predictions, recent_predictions)
            return float(ks_stat)
        except Exception:
            return 0.0
    
    def monitor_prediction_distribution(
        self,
        tenant_id: Optional[str] = None,
    ) -> dict:
        """
        Monitor current prediction distribution against expected bounds.
        
        Checks:
        - Risk level distribution (should match historical patterns)
        - Score distribution (should be well-calibrated)
        - Extreme predictions (too many critical or too few)
        """
        # Get recent predictions (last 24 hours)
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(hours=24)
        
        predictions = self._get_predictions_in_range(
            tenant_id, start_date, end_date
        )
        
        if not predictions:
            return {
                "status": "insufficient_data",
                "message": "No predictions in last 24 hours",
                "total": 0,
            }
        
        scores = np.array([p["risk_score"] for p in predictions])
        levels = [p["risk_level"] for p in predictions]
        
        # Calculate distribution
        level_dist = {
            "low": levels.count("low") / len(levels),
            "moderate": levels.count("moderate") / len(levels),
            "high": levels.count("high") / len(levels),
            "critical": levels.count("critical") / len(levels),
        }
        
        # Check for anomalies
        issues = []
        
        if level_dist["critical"] > 0.20:
            issues.append({
                "type": "high_critical_rate",
                "message": f"Critical rate is {level_dist['critical']:.1%} (expected < 20%)",
                "severity": "warning",
            })
        
        if level_dist["low"] > 0.95:
            issues.append({
                "type": "high_low_rate",
                "message": f"Low risk rate is {level_dist['low']:.1%} - model may be under-predicting",
                "severity": "warning",
            })
        
        # Check score distribution (should be somewhat uniform, not all at extremes)
        score_std = np.std(scores)
        if score_std < 0.1:
            issues.append({
                "type": "low_variance",
                "message": f"Score standard deviation is {score_std:.3f} - predictions may lack discrimination",
                "severity": "warning",
            })
        
        return {
            "status": "healthy" if not issues else "degraded",
            "total": len(predictions),
            "distribution": level_dist,
            "score_stats": {
                "mean": float(np.mean(scores)),
                "std": float(np.std(scores)),
                "min": float(np.min(scores)),
                "max": float(np.max(scores)),
                "median": float(np.median(scores)),
            },
            "issues": issues,
        }
    
    def track_performance_metrics(
        self,
        tenant_id: Optional[str] = None,
        lookback_days: int = 30,
    ) -> dict:
        """
        Track model performance over time using actual outcomes.
        
        Compares predictions against actual outcomes (students who fell behind)
        to calculate accuracy, precision, recall.
        
        Note: Requires outcome data to be available (lagged by ~30 days).
        """
        end_date = datetime.now(timezone.utc) - timedelta(days=30)  # Lagged for outcomes
        start_date = end_date - timedelta(days=lookback_days)
        
        # Get predictions with outcomes
        data = self._get_predictions_with_outcomes(
            tenant_id, start_date, end_date
        )
        
        if not data or len(data) < 100:
            return {
                "status": "insufficient_data",
                "message": f"Only {len(data) if data else 0} predictions with outcomes available",
            }
        
        # Calculate metrics
        y_true = np.array([d["actual_outcome"] for d in data])
        y_pred = np.array([d["predicted_risk"] >= 0.5 for d in data])
        y_scores = np.array([d["predicted_risk"] for d in data])
        
        from sklearn.metrics import (
            accuracy_score,
            precision_score,
            recall_score,
            roc_auc_score,
        )
        
        metrics = {
            "accuracy": float(accuracy_score(y_true, y_pred)),
            "precision": float(precision_score(y_true, y_pred, zero_division=0)),
            "recall": float(recall_score(y_true, y_pred, zero_division=0)),
            "auc_roc": float(roc_auc_score(y_true, y_scores)),
            "sample_size": len(data),
            "period_start": start_date.isoformat(),
            "period_end": end_date.isoformat(),
        }
        
        # Check against thresholds
        issues = []
        if metrics["auc_roc"] < 0.70:
            issues.append({
                "metric": "auc_roc",
                "value": metrics["auc_roc"],
                "threshold": 0.70,
                "message": "AUC-ROC below acceptable threshold",
            })
        
        if metrics["recall"] < 0.75:
            issues.append({
                "metric": "recall",
                "value": metrics["recall"],
                "threshold": 0.75,
                "message": "Recall below acceptable threshold - may be missing at-risk students",
            })
        
        metrics["issues"] = issues
        metrics["status"] = "healthy" if not issues else "degraded"
        
        # Store metrics
        self._store_performance_metrics(metrics)
        
        return metrics
    
    # Health check helpers
    
    def _check_model_availability(self) -> HealthCheckResult:
        """Check if model is loaded and responding"""
        try:
            # Try a dummy prediction
            # This would call the actual model
            return HealthCheckResult(
                check_name="model_availability",
                status="healthy",
                message="Model is loaded and responding",
            )
        except Exception as e:
            return HealthCheckResult(
                check_name="model_availability",
                status="unhealthy",
                message=f"Model unavailable: {str(e)}",
            )
    
    def _check_prediction_latency(self) -> HealthCheckResult:
        """Check prediction latency"""
        # This would measure actual prediction time
        latency_ms = 50  # Placeholder
        
        if latency_ms > self.MAX_PREDICTION_LATENCY_MS:
            return HealthCheckResult(
                check_name="prediction_latency",
                status="degraded",
                message=f"High latency: {latency_ms}ms (threshold: {self.MAX_PREDICTION_LATENCY_MS}ms)",
                details={"latency_ms": latency_ms},
            )
        
        return HealthCheckResult(
            check_name="prediction_latency",
            status="healthy",
            message=f"Latency OK: {latency_ms}ms",
            details={"latency_ms": latency_ms},
        )
    
    def _check_database_health(self) -> HealthCheckResult:
        """Check database connectivity"""
        try:
            # Ping database
            return HealthCheckResult(
                check_name="database",
                status="healthy",
                message="Database connection OK",
            )
        except Exception as e:
            return HealthCheckResult(
                check_name="database",
                status="unhealthy",
                message=f"Database error: {str(e)}",
            )
    
    def _check_prediction_volume(self) -> HealthCheckResult:
        """Check that predictions are being generated"""
        # Count predictions in last 24 hours
        count = self._count_recent_predictions(hours=24)
        
        if count < self.MIN_DAILY_PREDICTIONS:
            return HealthCheckResult(
                check_name="prediction_volume",
                status="degraded",
                message=f"Low prediction volume: {count} in 24h (expected >= {self.MIN_DAILY_PREDICTIONS})",
                details={"count": count},
            )
        
        return HealthCheckResult(
            check_name="prediction_volume",
            status="healthy",
            message=f"Prediction volume OK: {count} in 24h",
            details={"count": count},
        )
    
    def _check_feature_store(self) -> HealthCheckResult:
        """Check feature store availability"""
        try:
            # Check feature store connection
            return HealthCheckResult(
                check_name="feature_store",
                status="healthy",
                message="Feature store available",
            )
        except Exception as e:
            return HealthCheckResult(
                check_name="feature_store",
                status="unhealthy",
                message=f"Feature store error: {str(e)}",
            )
    
    # Database helpers (placeholders)
    
    def _get_recent_prediction_data(
        self,
        tenant_id: Optional[str],  # noqa: ARG002
        start_date: datetime,  # noqa: ARG002
        end_date: datetime,  # noqa: ARG002
    ) -> Optional[dict]:
        """Get recent prediction data for drift analysis"""
        return None
    
    def _get_baseline_data(self, tenant_id: Optional[str]) -> Optional[dict]:  # noqa: ARG002
        """Get baseline data from model training"""
        return None
    
    def _get_predictions_in_range(
        self,
        tenant_id: Optional[str],  # noqa: ARG002
        start_date: datetime,  # noqa: ARG002
        end_date: datetime,  # noqa: ARG002
    ) -> list[dict]:
        """Get predictions in date range"""
        return []
    
    def _get_predictions_with_outcomes(
        self,
        tenant_id: Optional[str],  # noqa: ARG002
        start_date: datetime,  # noqa: ARG002
        end_date: datetime,  # noqa: ARG002
    ) -> list[dict]:
        """Get predictions with actual outcomes"""
        return []
    
    def _count_recent_predictions(
        self,
        hours: int,  # noqa: ARG002
    ) -> int:
        """Count predictions in recent hours"""
        return 0
    
    def _store_drift_report(
        self,
        report: DriftReport,  # noqa: ARG002
    ) -> None:
        """Store drift report"""
        pass
    
    def _store_performance_metrics(
        self,
        metrics: dict,  # noqa: ARG002
    ) -> None:
        """Store performance metrics"""
        pass
    
    def _send_alert(
        self,
        alert: MonitoringAlert,  # noqa: ARG002
    ) -> None:
        """Send monitoring alert"""
        pass
